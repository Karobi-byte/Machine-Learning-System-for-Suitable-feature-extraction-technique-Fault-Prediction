# -*- coding: utf-8 -*-
"""suitable feature extraction technique.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10hhy_qTqJdRcs-IZzPXTNYDr5u5YCfGR
"""

import pandas as pd

# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices
import numpy as np

# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy
import matplotlib.pyplot as plt

# 'Seaborn' is based on matplotlib; used for plotting statistical graphics
import seaborn as sns

# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# 'Statsmodels' is used to build and analyze various statistical models
import statsmodels
import statsmodels.api as sm
from statsmodels.tools.eval_measures import rmse
from statsmodels.formula.api import ols
from statsmodels.stats.outliers_influence import variance_inflation_factor

# 'SciPy' is used to perform scientific computations
from scipy.stats import shapiro
from scipy import stats

# import functions to perform feature selection
#from mlxtend.feature_selection import SequentialFeatureSelector as sfs

#import functions for time series
import itertools
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import tensorflow as tf
import xgboost as xgb
import os
import warnings

from google.colab import drive
import os
drive.mount('/content/drive')

import os

# Get the list of files in the current directory
file_list = os.listdir()
drive_path= "/content/drive/MyDrive/Thesis/csv"
# Print the list of file names
print(drive_path)
df_auto= pd.read_csv("/content/drive/MyDrive/Thesis/csv/Auto-regression.csv")
df_com= pd.read_csv("/content/drive/MyDrive/Thesis/csv/COM (1).csv")
df_gra=pd.read_csv("/content/drive/MyDrive/Thesis/csv/Grediant_1.csv")
df_his=pd.read_csv("/content/drive/MyDrive/Thesis/csv/Histrogram_1.csv")
df_rlm=pd.read_csv("/content/drive/MyDrive/Thesis/csv/RLM (2).csv")

# Display the DataFrame
print(df_auto.head())
print(df_com.head())
print(df_gra.head())
print(df_his.head())
print(df_rlm.head())

labels_auto = df_auto.iloc[:,-1].values  # Assuming labels are in the last column
features_auto = df_auto.iloc[:, :-1].values  # Assuming features start from the second column
labels_com = df_com.iloc[:,-1].values  # Assuming labels are in the first column
features_com = df_com.iloc[:, :-1].values  # Assuming features start from the second column
labels_gra = df_gra.iloc[:,-1].values  # Assuming labels are in the first column
features_gra = df_gra.iloc[:, :-1].values  # Assuming features start from the second column
labels_his = df_his.iloc[:,-1].values  # Assuming labels are in the first column
features_his = df_his.iloc[:, :-1].values  # Assuming features start from the second column
labels_rlm = df_rlm.iloc[:,-1].values  # Assuming labels are in the first column
features_rlm = df_rlm.iloc[:, :-1].values  # Assuming features start from the second column

labels_auto_df = pd.DataFrame({'label': labels_auto})
features_auto_df = pd.DataFrame(features_auto, columns=df_auto.columns[:-1])


labels_com_df = pd.DataFrame({'label': labels_com})
features_com_df = pd.DataFrame(features_com, columns=df_com.columns[:-1])

labels_gra_df = pd.DataFrame({'label': labels_gra})
features_gra_df = pd.DataFrame(features_gra, columns=df_gra.columns[:-1])

labels_his_df = pd.DataFrame({'label': labels_his})
features_his_df = pd.DataFrame(features_his, columns=df_his.columns[:-1])

labels_rlm_df = pd.DataFrame({'label': labels_rlm})
features_rlm_df = pd.DataFrame(features_rlm, columns=df_rlm.columns[:-1])

# Save each subset's labels and features to separate CSV files
labels_auto_df.to_csv(drive_path+'labels_auto.csv', index=False)
features_auto_df.to_csv(drive_path+'features_auto.csv', index=False)

labels_com_df.to_csv(drive_path+'labels_com.csv', index=False)
features_com_df.to_csv(drive_path+'features_com.csv', index=False)

labels_gra_df.to_csv(drive_path+'labels_gra.csv', index=False)
features_gra_df.to_csv('features_gra.csv', index=False)

labels_his_df.to_csv('labels_his.csv', index=False)
features_his_df.to_csv('features_his.csv', index=False)

labels_rlm_df.to_csv('labels_rlm.csv', index=False)
features_rlm_df.to_csv('features_rlm.csv', index=False)


print("Labels:")
print(labels_auto)

print("Labels:")
print(labels_com)

print("\nFeatures:")
print(features_auto)

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

# Load your features and labels data for the "auto" subset
labels_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_auto.csv')
features_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_auto.csv')

labels_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_com.csv')
features_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_com.csv')

labels_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_gra.csv')
features_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_gra.csv')

labels_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_his.csv')
features_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_his.csv')

labels_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_rlm.csv')
features_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_rlm.csv')

# Split the data into training and testing sets
X_auto = features_auto_df.values  # Features
y_auto = labels_auto_df['label'].values  # Labels
X_com = features_com_df.values  # Features
y_com = labels_com_df['label'].values  # Labels
X_gra = features_gra_df.values  # Features
y_gra = labels_gra_df['label'].values  # Labels
X_his = features_his_df.values  # Features
y_his = labels_his_df['label'].values  # Labels
X_rlm = features_rlm_df.values  # Features
y_rlm = labels_rlm_df['label'].values  # Labels

# Split the data into training (80%) and testing (20%) sets
X_train_auto, X_test_auto, y_train_auto, y_test_auto = train_test_split(
    X_auto, y_auto, test_size=0.2, random_state=42
)


X_train_com, X_test_com, y_train_com, y_test_com = train_test_split(
    features_com, labels_com, test_size=0.2, random_state=42
)

X_train_gra, X_test_gra, y_train_gra, y_test_gra = train_test_split(
    features_gra, labels_gra, test_size=0.2, random_state=42
)

X_train_his, X_test_his, y_train_his, y_test_his = train_test_split(
    features_his, labels_his, test_size=0.2, random_state=42
)

X_train_rlm, X_test_rlm, y_train_rlm, y_test_rlm = train_test_split(
    features_rlm, labels_rlm, test_size=0.2, random_state=42
)

# Create a Gradient Boosting Classifier for each subset
gb_auto = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_com = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_gra = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_his = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_rlm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the models on the training data for each subset
gb_auto.fit(X_train_auto, y_train_auto)
gb_com.fit(X_train_com, y_train_com)
gb_gra.fit(X_train_gra, y_train_gra)
gb_his.fit(X_train_his, y_train_his)
gb_rlm.fit(X_train_rlm, y_train_rlm)

from sklearn.metrics import accuracy_score, classification_report

# Make predictions on the testing data for each subset
y_pred_auto = gb_auto.predict(X_test_auto)
y_pred_com = gb_com.predict(X_test_com)
y_pred_gra = gb_gra.predict(X_test_gra)
y_pred_his = gb_his.predict(X_test_his)
y_pred_rlm = gb_rlm.predict(X_test_rlm)

# Evaluate the models for each subset
accuracy_auto = accuracy_score(y_test_auto, y_pred_auto)
classification_rep_auto = classification_report(y_test_auto, y_pred_auto)

accuracy_com = accuracy_score(y_test_com, y_pred_com)
classification_rep_com = classification_report(y_test_com, y_pred_com)

accuracy_gra = accuracy_score(y_test_gra, y_pred_gra)
classification_rep_gra = classification_report(y_test_gra, y_pred_gra)

accuracy_his = accuracy_score(y_test_his, y_pred_his)
classification_rep_his = classification_report(y_test_his, y_pred_his)

accuracy_rlm = accuracy_score(y_test_rlm, y_pred_rlm)
classification_rep_rlm = classification_report(y_test_rlm, y_pred_rlm)

# Print the evaluation results for each subset
print("Auto Subset - Accuracy:", accuracy_auto)
print("Auto Subset - Classification Report:\n", classification_rep_auto)

print("Com Subset - Accuracy:", accuracy_com)
print("Com Subset - Classification Report:\n", classification_rep_com)

print("Gra Subset - Accuracy:", accuracy_gra)
print("Gra Subset - Classification Report:\n", classification_rep_gra)

print("His Subset - Accuracy:", accuracy_his)
print("His Subset - Classification Report:\n", classification_rep_his)

print("Rlm Subset - Accuracy:", accuracy_rlm)
print("Rlm Subset - Classification Report:\n", classification_rep_rlm)

import pandas as pd
import numpy as np

drive_path='/content/drive/MyDrive/Thesis/csv'
# Calculate mean and standard deviation for each subset of features
mean_auto = np.mean(features_auto, axis=0)
std_auto = np.std(features_auto, axis=0)

mean_com = np.mean(features_com, axis=0)
std_com = np.std(features_com, axis=0)

mean_gra = np.mean(features_gra, axis=0)
std_gra = np.std(features_gra, axis=0)

mean_his=np.mean(features_his, axis=0)
std_his = np.std(features_his, axis=0)

mean_rlm=np.mean(features_rlm, axis=0)
std_rlm=np.std(features_rlm, axis=0)


# Normalize each subset of features
norm_dfauto = (features_auto - mean_auto) / std_auto
norm_dfcom = (features_com - mean_com) / std_com
norm_dfgra = (features_gra - mean_gra) / std_gra
norm_dfhis = (features_his - mean_his) / std_his
norm_dfrlm = (features_rlm - mean_rlm) / std_rlm

column_names_com = [f"feature{i}" for i in range(1, 45)]
column_names_his = [f"feature{i}" for i in range(1, 10)]
column_names_rlm = [f"feature{i}" for i in range(1, 21)]
# Convert normalized data to pandas DataFrames
norm_auto = pd.DataFrame(norm_dfauto, columns=["feature1", "feature2", "feature3", "feature4", "feature5"])  # Replace with actual column names
norm_com = pd.DataFrame(norm_dfcom, columns=column_names_com)    # Replace with actual column names
norm_gra = pd.DataFrame(norm_dfgra, columns=["feature1", "feature2", "feature3", "feature4", "feature5"])    # Replace with actual column names
norm_his = pd.DataFrame(norm_dfhis, columns=column_names_his)    # Replace with actual column names
norm_rlm = pd.DataFrame(norm_dfrlm, columns= column_names_rlm)

# Save the normalized data to new CSV files

norm_auto.to_csv(drive_path+'normalized_auto.csv', index=False)
norm_com.to_csv(drive_path+'normalized_com.csv', index=False)
norm_gra.to_csv(drive_path+'normalized_gra.csv', index=False)
norm_his.to_csv(drive_path+'normalized_his.csv', index=False)
norm_rlm.to_csv(drive_path+'normalized_rlm.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_auto.csv')
norm_feat_auto = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_auto.csv')

labels_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_com.csv')
norm_feat_com = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_com.csv')

labels_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_gra.csv')
norm_feat_gra = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_gra.csv')

labels_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_his.csv')
norm_feat_his = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_his.csv')

labels_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_rlm.csv')
norm_feat_rlm = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_rlm.csv')

# Split each subset into training and testing sets
X_train_norm_auto, X_test_norm_auto, y_train_auto, y_test_auto = train_test_split(
    norm_feat_auto, labels_auto_df['label'], test_size=0.2, random_state=42
)

X_train_norm_com, X_test_norm_com, y_train_com, y_test_com = train_test_split(
    norm_feat_com, labels_com_df['label'], test_size=0.2, random_state=42
)

X_train_norm_gra, X_test_norm_gra, y_train_gra, y_test_gra = train_test_split(
    norm_feat_gra, labels_gra_df['label'], test_size=0.2, random_state=42
)

X_train_norm_his, X_test_norm_his, y_train_his, y_test_his = train_test_split(
    norm_feat_his, labels_his_df['label'], test_size=0.2, random_state=42
)

X_train_norm_rlm, X_test_norm_rlm, y_train_rlm, y_test_rlm = train_test_split(
    norm_feat_rlm, labels_rlm_df['label'], test_size=0.2, random_state=42
)

# Create Gradient Boosting Classifier instances for each subset
gb_auto = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_com = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_gra = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_his = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_rlm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the models on the training data for each subset
gb_auto.fit(X_train_norm_auto, y_train_auto)
gb_com.fit(X_train_norm_com, y_train_com)
gb_gra.fit(X_train_norm_gra, y_train_gra)
gb_his.fit(X_train_norm_his, y_train_his)
gb_rlm.fit(X_train_norm_rlm, y_train_rlm)

# Make predictions on the testing data for each subset
y_pred_auto = gb_auto.predict(X_test_norm_auto)
y_pred_com = gb_com.predict(X_test_norm_com)
y_pred_gra = gb_gra.predict(X_test_norm_gra)
y_pred_his = gb_his.predict(X_test_norm_his)
y_pred_rlm = gb_rlm.predict(X_test_norm_rlm)

# Evaluate the models for each subset
accuracy_auto = accuracy_score(y_test_auto, y_pred_auto)
classification_rep_auto = classification_report(y_test_auto, y_pred_auto)

accuracy_com = accuracy_score(y_test_com, y_pred_com)
classification_rep_com = classification_report(y_test_com, y_pred_com)

accuracy_gra = accuracy_score(y_test_gra, y_pred_gra)
classification_rep_gra = classification_report(y_test_gra, y_pred_gra)

accuracy_his = accuracy_score(y_test_his, y_pred_his)
classification_rep_his = classification_report(y_test_his, y_pred_his)

accuracy_rlm = accuracy_score(y_test_rlm, y_pred_rlm)
classification_rep_rlm = classification_report(y_test_rlm, y_pred_rlm)

# Print the evaluation results for each subset
print("Auto Subset - Accuracy:", accuracy_auto)
print("Auto Subset - Classification Report:\n", classification_rep_auto)

print("Com Subset - Accuracy:", accuracy_com)
print("Com Subset - Classification Report:\n", classification_rep_com)

print("Gra Subset - Accuracy:", accuracy_gra)
print("Gra Subset - Classification Report:\n", classification_rep_gra)

print("His Subset - Accuracy:", accuracy_his)
print("His Subset - Classification Report:\n", classification_rep_his)

print("Rlm Subset - Accuracy:", accuracy_rlm)
print("Rlm Subset - Classification Report:\n", classification_rep_rlm)

"""NN"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_auto.csv')
norm_feat_auto = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_auto.csv')

# Convert DataFrames to NumPy arrays
X_norm_auto = norm_feat_auto.values
y_auto = labels_auto_df['label'].values

# Split the data into training and testing sets
X_train_norm_auto, X_test_norm_auto, y_train_auto, y_test_auto = train_test_split(
    X_norm_auto, y_auto, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_auto = torch.tensor(X_train_norm_auto, dtype=torch.float32)
y_train_tensor_auto = torch.tensor(y_train_auto, dtype=torch.long)
X_test_tensor_auto = torch.tensor(X_test_norm_auto, dtype=torch.float32)
y_test_tensor_auto = torch.tensor(y_test_auto, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_auto.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_auto = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_auto = nn.CrossEntropyLoss()
optimizer_auto = optim.Adam(model_auto.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_auto.zero_grad()
    outputs_auto = model_auto(X_train_tensor_auto)
    loss_auto = criterion_auto(outputs_auto, y_train_tensor_auto)
    loss_auto.backward()
    optimizer_auto.step()

# Evaluate the model
with torch.no_grad():
    model_auto.eval()
    y_pred_auto = model_auto(X_test_tensor_auto).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_auto = accuracy_score(y_test_auto, y_pred_auto)
classification_rep_auto = classification_report(y_test_auto, y_pred_auto)

# Print evaluation results
print("Auto Subset - Accuracy:", accuracy_auto)
print("Auto Subset - Classification Report:\n", classification_rep_auto)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_com.csv')
norm_feat_com = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_com.csv')

# Convert DataFrames to NumPy arrays
X_norm_com = norm_feat_com.values
y_com = labels_com_df['label'].values

# Split the data into training and testing sets
X_train_norm_com, X_test_norm_com, y_train_com, y_test_com = train_test_split(
    X_norm_com, y_com, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_com = torch.tensor(X_train_norm_com, dtype=torch.float32)
y_train_tensor_com = torch.tensor(y_train_com, dtype=torch.long)
X_test_tensor_com = torch.tensor(X_test_norm_com, dtype=torch.float32)
y_test_tensor_com = torch.tensor(y_test_com, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_com.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(X_train_tensor_com)
    loss = criterion(outputs, y_train_tensor_com)
    loss.backward()
    optimizer.step()

# Evaluate the model
with torch.no_grad():
    model.eval()
    y_pred_com = model(X_test_tensor_com).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_com = accuracy_score(y_test_com, y_pred_com)
classification_rep_com = classification_report(y_test_com, y_pred_com)

# Print evaluation results
print("Com Subset - Accuracy:", accuracy_com)
print("Com Subset - Classification Report:\n", classification_rep_com)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_gra.csv')
norm_feat_gra = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_gra.csv')

# Convert DataFrames to NumPy arrays
X_norm_gra = norm_feat_gra.values
y_gra = labels_gra_df['label'].values

# Split the data into training and testing sets
X_train_norm_gra, X_test_norm_gra, y_train_gra, y_test_gra = train_test_split(
    X_norm_gra, y_gra, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_gra = torch.tensor(X_train_norm_gra, dtype=torch.float32)
y_train_tensor_gra = torch.tensor(y_train_gra, dtype=torch.long)
X_test_tensor_gra = torch.tensor(X_test_norm_gra, dtype=torch.float32)
y_test_tensor_gra = torch.tensor(y_test_gra, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_gra.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_gra = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_gra = nn.CrossEntropyLoss()
optimizer_gra = optim.Adam(model_gra.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_gra.zero_grad()
    outputs_gra = model_gra(X_train_tensor_gra)
    loss_gra = criterion_gra(outputs_gra, y_train_tensor_gra)
    loss_gra.backward()
    optimizer_gra.step()

# Evaluate the model
with torch.no_grad():
    model_gra.eval()
    y_pred_gra = model_gra(X_test_tensor_gra).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_gra = accuracy_score(y_test_gra, y_pred_gra)
classification_rep_gra = classification_report(y_test_gra, y_pred_gra)

# Print evaluation results
print("Gra Subset - Accuracy:", accuracy_gra)
print("Gra Subset - Classification Report:\n", classification_rep_gra)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_his.csv')
norm_feat_his = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_his.csv')

# Convert DataFrames to NumPy arrays
X_norm_his = norm_feat_his.values
y_his = labels_his_df['label'].values

# Split the data into training and testing sets
X_train_norm_his, X_test_norm_his, y_train_his, y_test_his = train_test_split(
    X_norm_his, y_his, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_his = torch.tensor(X_train_norm_his, dtype=torch.float32)
y_train_tensor_his = torch.tensor(y_train_his, dtype=torch.long)
X_test_tensor_his = torch.tensor(X_test_norm_his, dtype=torch.float32)
y_test_tensor_his = torch.tensor(y_test_his, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_his.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_his = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_his = nn.CrossEntropyLoss()
optimizer_his = optim.Adam(model_his.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_his.zero_grad()
    outputs_his = model_his(X_train_tensor_his)
    loss_his = criterion_his(outputs_his, y_train_tensor_his)
    loss_his.backward()
    optimizer_his.step()

# Evaluate the model
with torch.no_grad():
    model_his.eval()
    y_pred_his = model_his(X_test_tensor_his).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_his = accuracy_score(y_test_his, y_pred_his)
classification_rep_his = classification_report(y_test_his, y_pred_his)

# Print evaluation results
print("His Subset - Accuracy:", accuracy_his)
print("His Subset - Classification Report:\n", classification_rep_his)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_rlm.csv')
norm_feat_rlm = pd.read_csv('/content/drive/MyDrive/Thesis/csvnormalized_rlm.csv')

# Convert DataFrames to NumPy arrays
X_norm_rlm = norm_feat_rlm.values
y_rlm = labels_rlm_df['label'].values

# Split the data into training and testing sets
X_train_norm_rlm, X_test_norm_rlm, y_train_rlm, y_test_rlm = train_test_split(
    X_norm_rlm, y_rlm, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_rlm = torch.tensor(X_train_norm_rlm, dtype=torch.float32)
y_train_tensor_rlm = torch.tensor(y_train_rlm, dtype=torch.long)
X_test_tensor_rlm = torch.tensor(X_test_norm_rlm, dtype=torch.float32)
y_test_tensor_rlm = torch.tensor(y_test_rlm, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_rlm.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_rlm = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_rlm = nn.CrossEntropyLoss()
optimizer_rlm = optim.Adam(model_rlm.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_rlm.zero_grad()
    outputs_rlm = model_rlm(X_train_tensor_rlm)
    loss_rlm = criterion_rlm(outputs_rlm, y_train_tensor_rlm)
    loss_rlm.backward()
    optimizer_rlm.step()

# Evaluate the model
with torch.no_grad():
    model_rlm.eval()
    y_pred_rlm = model_rlm(X_test_tensor_rlm).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_rlm = accuracy_score(y_test_rlm, y_pred_rlm)
classification_rep_rlm = classification_report(y_test_rlm, y_pred_rlm)

# Print evaluation results
print("Rlm Subset - Accuracy:", accuracy_rlm)
print("Rlm Subset - Classification Report:\n", classification_rep_rlm)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_auto.csv')
features_auto_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_auto.csv')

# Convert DataFrames to NumPy arrays
X_norm_auto = features_auto_df.values
y_auto = labels_auto_df['label'].values

# Split the data into training and testing sets
X_train_norm_auto, X_test_norm_auto, y_train_auto, y_test_auto = train_test_split(
    X_norm_auto, y_auto, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_auto = torch.tensor(X_train_norm_auto, dtype=torch.float32)
y_train_tensor_auto = torch.tensor(y_train_auto, dtype=torch.long)
X_test_tensor_auto = torch.tensor(X_test_norm_auto, dtype=torch.float32)
y_test_tensor_auto = torch.tensor(y_test_auto, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_auto.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_auto = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_auto = nn.CrossEntropyLoss()
optimizer_auto = optim.Adam(model_auto.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_auto.zero_grad()
    outputs_auto = model_auto(X_train_tensor_auto)
    loss_auto = criterion_auto(outputs_auto, y_train_tensor_auto)
    loss_auto.backward()
    optimizer_auto.step()

# Evaluate the model
with torch.no_grad():
    model_auto.eval()
    y_pred_auto = model_auto(X_test_tensor_auto).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_auto = accuracy_score(y_test_auto, y_pred_auto)
classification_rep_auto = classification_report(y_test_auto, y_pred_auto)

# Print evaluation results
print("Auto Subset - Accuracy:", accuracy_auto)
print("Auto Subset - Classification Report:\n", classification_rep_auto)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_com.csv')
features_com_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_com.csv')

# Convert DataFrames to NumPy arrays
X_norm_com = features_com_df.values
y_com = labels_com_df['label'].values

# Split the data into training and testing sets
X_train_norm_com, X_test_norm_com, y_train_com, y_test_com = train_test_split(
    X_norm_com, y_com, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_com = torch.tensor(X_train_norm_com, dtype=torch.float32)
y_train_tensor_com = torch.tensor(y_train_com, dtype=torch.long)
X_test_tensor_com = torch.tensor(X_test_norm_com, dtype=torch.float32)
y_test_tensor_com = torch.tensor(y_test_com, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_com.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_com = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_com = nn.CrossEntropyLoss()
optimizer_com = optim.Adam(model_com.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_com.zero_grad()
    outputs_com = model_com(X_train_tensor_com)
    loss_com = criterion_com(outputs_com, y_train_tensor_com)
    loss_com.backward()
    optimizer_com.step()

# Evaluate the model
with torch.no_grad():
    model_com.eval()
    y_pred_com = model_com(X_test_tensor_com).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_com = accuracy_score(y_test_com, y_pred_com)
classification_rep_com = classification_report(y_test_com, y_pred_com)

# Print evaluation results
print("Com Subset - Accuracy:", accuracy_com)
print("Com Subset - Classification Report:\n", classification_rep_com)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_gra.csv')
features_gra_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_gra.csv')

# Convert DataFrames to NumPy arrays
X_norm_gra = features_gra_df.values
y_gra = labels_gra_df['label'].values

# Split the data into training and testing sets
X_train_norm_gra, X_test_norm_gra, y_train_gra, y_test_gra = train_test_split(
    X_norm_gra, y_gra, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_gra = torch.tensor(X_train_norm_gra, dtype=torch.float32)
y_train_tensor_gra = torch.tensor(y_train_gra, dtype=torch.long)
X_test_tensor_gra = torch.tensor(X_test_norm_gra, dtype=torch.float32)
y_test_tensor_gra = torch.tensor(y_test_gra, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_gra.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_gra = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_gra = nn.CrossEntropyLoss()
optimizer_gra = optim.Adam(model_gra.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_gra.zero_grad()
    outputs_gra = model_gra(X_train_tensor_gra)
    loss_gra = criterion_gra(outputs_gra, y_train_tensor_gra)
    loss_gra.backward()
    optimizer_gra.step()

# Evaluate the model
with torch.no_grad():
    model_gra.eval()
    y_pred_gra = model_gra(X_test_tensor_gra).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_gra = accuracy_score(y_test_gra, y_pred_gra)
classification_rep_gra = classification_report(y_test_gra, y_pred_gra)

# Print evaluation results
print("Gra Subset - Accuracy:", accuracy_gra)
print("Gra Subset - Classification Report:\n", classification_rep_gra)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_his.csv')
features_his_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_his.csv')

# Convert DataFrames to NumPy arrays
X_norm_his = features_his_df.values
y_his = labels_his_df['label'].values

# Split the data into training and testing sets
X_train_norm_his, X_test_norm_his, y_train_his, y_test_his = train_test_split(
    X_norm_his, y_his, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_his = torch.tensor(X_train_norm_his, dtype=torch.float32)
y_train_tensor_his = torch.tensor(y_train_his, dtype=torch.long)
X_test_tensor_his = torch.tensor(X_test_norm_his, dtype=torch.float32)
y_test_tensor_his = torch.tensor(y_test_his, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_his.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_his = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_his = nn.CrossEntropyLoss()
optimizer_his = optim.Adam(model_his.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_his.zero_grad()
    outputs_his = model_his(X_train_tensor_his)
    loss_his = criterion_his(outputs_his, y_train_tensor_his)
    loss_his.backward()
    optimizer_his.step()

# Evaluate the model
with torch.no_grad():
    model_his.eval()
    y_pred_his = model_his(X_test_tensor_his).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_his = accuracy_score(y_test_his, y_pred_his)
classification_rep_his = classification_report(y_test_his, y_pred_his)

# Print evaluation results
print("His Subset - Accuracy:", accuracy_his)
print("His Subset - Classification Report:\n", classification_rep_his)

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load normalized features and labels from CSV files
labels_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvlabels_rlm.csv')
features_rlm_df = pd.read_csv('/content/drive/MyDrive/Thesis/csvfeatures_rlm.csv')

# Convert DataFrames to NumPy arrays
X_norm_rlm = features_rlm_df.values
y_rlm = labels_rlm_df['label'].values

# Split the data into training and testing sets
X_train_norm_rlm, X_test_norm_rlm, y_train_rlm, y_test_rlm = train_test_split(
    X_norm_rlm, y_rlm, test_size=0.2, random_state=42
)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor_rlm = torch.tensor(X_train_norm_rlm, dtype=torch.float32)
y_train_tensor_rlm = torch.tensor(y_train_rlm, dtype=torch.long)
X_test_tensor_rlm = torch.tensor(X_test_norm_rlm, dtype=torch.float32)
y_test_tensor_rlm = torch.tensor(y_test_rlm, dtype=torch.long)

# Define a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Initialize the neural network
input_size = X_train_norm_rlm.shape[1]
hidden_size = 128
output_size = 2  # Number of classes (0 and 1)
model_rlm = NeuralNetwork(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion_rlm = nn.CrossEntropyLoss()
optimizer_rlm = optim.Adam(model_rlm.parameters(), lr=0.0001)

# Train the neural network
num_epochs = 500
for epoch in range(num_epochs):
    optimizer_rlm.zero_grad()
    outputs_rlm = model_rlm(X_train_tensor_rlm)
    loss_rlm = criterion_rlm(outputs_rlm, y_train_tensor_rlm)
    loss_rlm.backward()
    optimizer_rlm.step()

# Evaluate the model
with torch.no_grad():
    model_rlm.eval()
    y_pred_rlm = model_rlm(X_test_tensor_rlm).argmax(dim=1).numpy()

# Calculate accuracy and classification report
accuracy_rlm = accuracy_score(y_test_rlm, y_pred_rlm)
classification_rep_rlm = classification_report(y_test_rlm, y_pred_rlm)

# Print evaluation results
print("Rlm Subset - Accuracy:", accuracy_rlm)
print("Rlm Subset - Classification Report:\n", classification_rep_rlm)